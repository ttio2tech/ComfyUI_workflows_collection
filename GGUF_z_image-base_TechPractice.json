{
  "id": "92112d97-bb64-4b44-86f2-ea5691ef8f6e",
  "revision": 0,
  "last_node_id": 32,
  "last_link_id": 53,
  "nodes": [
    {
      "id": 17,
      "type": "VAELoader",
      "pos": [
        71.22825209024009,
        614.4132208648346
      ],
      "size": [
        270,
        58
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "links": [
            45
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "VAELoader"
      },
      "widgets_values": [
        "ae.safetensors"
      ]
    },
    {
      "id": 13,
      "type": "EmptySD3LatentImage",
      "pos": [
        530,
        620
      ],
      "size": [
        315,
        106
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "slot_index": 0,
          "links": [
            17
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "EmptySD3LatentImage"
      },
      "widgets_values": [
        1024,
        1024,
        1
      ]
    },
    {
      "id": 7,
      "type": "CLIPTextEncode",
      "pos": [
        420,
        400
      ],
      "size": [
        425.27801513671875,
        180.6060791015625
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 44
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            6
          ]
        }
      ],
      "title": "CLIP Text Encode (Negative Prompt)",
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "blurry ugly bad"
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 18,
      "type": "CLIPLoader",
      "pos": [
        80,
        460
      ],
      "size": [
        270,
        106
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            43,
            44
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "CLIPLoader"
      },
      "widgets_values": [
        "qwen_3_4b.safetensors",
        "lumina2",
        "default"
      ]
    },
    {
      "id": 15,
      "type": "Note",
      "pos": [
        71.95149993896484,
        192.96051025390625
      ],
      "size": [
        319.26513671875,
        197.89625549316406
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "The \"You are an assistant... <Prompt Start> \" text before the actual prompt is the one used in the official example.\n\nThe reason it is exposed to the user like this is because the model still works if you modify or remove it."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 16,
      "type": "UNETLoader",
      "pos": [
        64.24042771870836,
        55.07513366209636
      ],
      "size": [
        270,
        82
      ],
      "flags": {},
      "order": 4,
      "mode": 4,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": []
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "UNETLoader"
      },
      "widgets_values": [
        "z_image_turbo_bf16.safetensors",
        "default"
      ]
    },
    {
      "id": 6,
      "type": "CLIPTextEncode",
      "pos": [
        420,
        190
      ],
      "size": [
        423.83001708984375,
        177.11770629882812
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 43
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            4
          ]
        }
      ],
      "title": "CLIP Text Encode (Positive Prompt)",
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "You are an assistant... <Prompt Start> A full-body studio portrait of a handsome young East Asian woman sitting on the floor in front of a light purple backdrop. She is wearing a cozy, oversized lavender chunky-knit sweater, white skirt, and white socks. She is affectionately hugging a large Sanrio Kuromi plushie and looking at the camera with a gentle expression. The background is decorated with playful, hand-drawn purple doodles and text, including \"T\", \"Tech Practice\", paper airplanes, and flowers, in the style of a K-pop photocard or fanzine cover. The lighting is bright and soft, enhancing the cute and affectionate mood. Use the exact same face as in the uploaded image, no alteration, 100% unchanged. Remove her earrings.\n"
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 11,
      "type": "ModelSamplingAuraFlow",
      "pos": [
        390,
        60
      ],
      "size": [
        315,
        58
      ],
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 53
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "slot_index": 0,
          "links": [
            47
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "ModelSamplingAuraFlow"
      },
      "widgets_values": [
        3
      ]
    },
    {
      "id": 8,
      "type": "VAEDecode",
      "pos": [
        1208.0403180211022,
        -22.178021040487394
      ],
      "size": [
        210,
        46
      ],
      "flags": {},
      "order": 11,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 51
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 45
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "slot_index": 0,
          "links": [
            16
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "VAEDecode"
      },
      "widgets_values": []
    },
    {
      "id": 9,
      "type": "SaveImage",
      "pos": [
        1438.6237268611544,
        -16.328861973101734
      ],
      "size": [
        976.0567626953125,
        1060.9766845703125
      ],
      "flags": {},
      "order": 12,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 16
        }
      ],
      "outputs": [],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0"
      },
      "widgets_values": [
        "ComfyUI"
      ]
    },
    {
      "id": 3,
      "type": "KSampler",
      "pos": [
        872.5972063937778,
        -9.782275883953524
      ],
      "size": [
        315,
        262
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 47
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 4
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 6
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 17
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "slot_index": 0,
          "links": [
            51
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.11.0",
        "Node name for S&R": "KSampler"
      },
      "widgets_values": [
        75996892794196,
        "randomize",
        25,
        4,
        "res_multistep",
        "simple",
        1
      ]
    },
    {
      "id": 31,
      "type": "UnetLoaderGGUF",
      "pos": [
        -127.85718274300619,
        -215.01720846306154
      ],
      "size": [
        474.8523693508172,
        70.78225606292833
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            53
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfyui-gguf",
        "ver": "1.1.9",
        "Node name for S&R": "UnetLoaderGGUF"
      },
      "widgets_values": [
        "z_image-Q4_K_M.gguf"
      ]
    },
    {
      "id": 32,
      "type": "MarkdownNote",
      "pos": [
        -459.9331941257819,
        -14.355652771229684
      ],
      "size": [
        510,
        420
      ],
      "flags": {
        "collapsed": false
      },
      "order": 6,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Model link (for local users)",
      "properties": {},
      "widgets_values": [
        "[Video](https://youtu.be/Dk51MdgujQU)\n\n## Model links\n\n**text_encoders**\n\n- [qwen_3_4b.safetensors](https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors)\n\n**diffusion_models**\n\n- [z_image-Q4_K_M.gguf](https://huggingface.co/unsloth/Z-Image-GGUF/resolve/main/z-image-Q4_K_M.gguf)\n- [z_image_bf16.safetensors](https://huggingface.co/Comfy-Org/z_image/resolve/main/split_files/diffusion_models/z_image_bf16.safetensors)\n\n**vae**\n\n- [ae.safetensors](https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors)\n\n\nModel Storage Location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚      â””â”€â”€ qwen_3_4b.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚      â””â”€â”€ z_image_bf16.safetensors\nâ”‚   â”‚      â””â”€â”€ z_image-Q4_K_M.gguf\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚          â””â”€â”€ ae.safetensors\n```\n"
      ],
      "color": "#432",
      "bgcolor": "#000"
    }
  ],
  "links": [
    [
      4,
      6,
      0,
      3,
      1,
      "CONDITIONING"
    ],
    [
      6,
      7,
      0,
      3,
      2,
      "CONDITIONING"
    ],
    [
      16,
      8,
      0,
      9,
      0,
      "IMAGE"
    ],
    [
      17,
      13,
      0,
      3,
      3,
      "LATENT"
    ],
    [
      43,
      18,
      0,
      6,
      0,
      "CLIP"
    ],
    [
      44,
      18,
      0,
      7,
      0,
      "CLIP"
    ],
    [
      45,
      17,
      0,
      8,
      1,
      "VAE"
    ],
    [
      47,
      11,
      0,
      3,
      0,
      "MODEL"
    ],
    [
      51,
      3,
      0,
      8,
      0,
      "LATENT"
    ],
    [
      53,
      31,
      0,
      11,
      0,
      "MODEL"
    ]
  ],
  "groups": [],
  "config": {},
  "extra": {
    "ds": {
      "scale": 0.5209868481924556,
      "offset": [
        871.5947550277889,
        622.149372863579
      ]
    },
    "frontendVersion": "1.37.11",
    "workflowRendererVersion": "LG",
    "VHS_latentpreview": false,
    "VHS_latentpreviewrate": 0,
    "VHS_MetadataImage": true,
    "VHS_KeepIntermediate": true
  },
  "version": 0.4
}